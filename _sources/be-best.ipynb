{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5c0fa7a4f8f1487a2aac67eb43e7b2e553808a81f9be50af9e1ab194481cfe22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Be best - Lab\n",
    "\n",
    "In this assignment we study exploration in the very abstract $n$-armed bandit ask. In this there are $n$ actions to take. Each returns a reward $R$, with some probability $p$. The reward value is either a 1 or a 0. This means the expected value of each arm is simply probability. Nice and simple right?\n",
    "\n",
    "Our agents are really learning, at last. Reinforcement learning, to be precise. \n",
    "\n",
    "The reward value $Q$ update rule for all agents (below) and arm is the same:\n",
    "\n",
    "$$ Q \\leftarrow Q + \\alpha * (R - Q) $$ [1]\n",
    "\n",
    "Where the learning rate _lr_reward_ is denoted as $\\alpha$, so that the equation above looks nice. The learning rate often gets called $\\alpha$ in the reinforcement learning world. If you are not familiar with the idea of a learning rate, it is what it sounds like. A parameter that controls how much each value update matters. This is, over time, the rate at which learning happens.\n",
    "\n",
    "Remember too that $Q$ is trying to approximate the average reward value of each arm.\n",
    "\n",
    "This kind of difference $(R - Q)$ in Eq [1] is typical of RL. If you're not sure what it means, consider in your head, what would happen to the value update if $Q$ was bigger than the reward $R$ (and overestimate), or if it was smaller. Once you have noodled that a bit, as needed, consider how making $\\alpha$ bigger or smaller might make $Q$ learning faster, or slower, or more or less volatile. (Learning speed and volatility _often_ go together; an annoying matched set.)\n",
    "\n",
    "_Note_: We are not going to use $\\alpha$ here. Just giving you some intuition. We will use it soon...\n",
    "\n",
    "Our exploration strategies are a random one, a sequential one, or $\\epsilon$-greedy (aka 'e'-greedy). \n",
    "\n",
    "The random one, and the sequential one, will come in two flavors. One that just explores, with no attempt to exploit. And one who's pure exploration is bound. Bounded by the number of steps of exploration that are allowed. Once that bound is exceeded, then both the Bounded agents will choose the arm they think is most valuable forevermore. Forevermore! \n",
    "\n",
    "The lab has two sections. \n",
    "\n",
    "- _First_ is a demo mostly. We will explore 4-armed bandits, and get to know our three basic agents.\n",
    "- _Second_, is where things get more interesting. We'll put $\\epsilon$=greedy, and the two bounded agents, in a little competition. The big question is this: tremendous effort has gone in over the years to study bandits when the exploration-exploitation dilemma is in play. The question this week is a doozy: was all work even necessary? \n",
    "\n",
    "Or to say it more directly: What is the cost in terms of rewards found to doing pure exploration, with a bound. \n",
    "\n",
    "In some ways this is not a fair competition in Section 2. Or really, in many ways. The $\\epsilon$-greedy method is not the best known solution to trading off exploration with exploitation. Then again, it is widely used to this day. It's a place to start to ask this, our big question, anyway. We'll come back to it next week, and try to be more fancy.\n",
    "\n",
    "Our metric is _total reward_. Maximizing that is the goal of all RL, afterall.\n",
    "\n",
    "## Install and import needed modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install explorationlib?\n",
    "!pip install --upgrade git+https://github.com/parenthetical-e/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import explorationlib\n",
    "\n",
    "from explorationlib.local_gym import BanditUniform4\n",
    "from explorationlib.agent import BanditActorCritic\n",
    "from explorationlib.agent import Critic\n",
    "from explorationlib.agent import EpsilonActor\n",
    "from explorationlib.agent import RandomActor\n",
    "from explorationlib.agent import SequentialActor\n",
    "from explorationlib.agent import BoundedRandomActor\n",
    "from explorationlib.agent import BoundedSequentialActor\n",
    "\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import action_entropy\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "from explorationlib.plot import plot_bandit\n",
    "from explorationlib.plot import plot_bandit_actions\n",
    "from explorationlib.plot import plot_bandit_critic\n",
    "from explorationlib.plot import plot_bandit_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.greedy=True\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = \"16\"\n",
    "\n",
    "# Dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "## Section 1 - Our friend, the bandit.\n",
    "\n",
    "In this section we'll study three explorers getting to know one bandit, with four arms."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The base case (Section 1)\n",
    "\n",
    "Let's make a four armed bandit and then plot its values. (Expected value is the term used in the literature, so I will too). \n",
    "\n",
    "_Note_: The random seed is fixed. If you change the see and run the cell below, some of the reward probabilities will change. The probability of the best arm, the optimal value arm is fixed however. It is set to 0.35, and located at arm 2. Try it! Rerun the cell below with different seeds, a few times, to get a sense of how the non-optimal arms can vary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared env params\n",
    "num_experiments = 1\n",
    "seed = 5030\n",
    "\n",
    "# Create env\n",
    "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
    "env.seed(seed)\n",
    "\n",
    "# Plot env\n",
    "plot_bandit(env, alpha=0.6)"
   ]
  },
  {
   "source": [
    "### Our three agents, _unbounded_\n",
    "\n",
    "A word about the code. Our agents this week work in whaat gets called an ActorCritic desgin. This breaks reinforcement learning algorithms in two two parts, obviously. The Actor does action selection. The Critic estimates the value of each action.\n",
    "\n",
    "Now in normal reinforcement learning, aka not pure exploration, the _Actor_ uses the $Q$ value estimates from the _Critic_ to, in part, make its decisions. Be it explore or exploit. This is indeed the case for the $\\epsilon$-greedy agent, _EpsilonActor_, works. \n",
    "\n",
    "...But... \n",
    "\n",
    "The other two agents--_SequentialActor_ and _RandomActor_--don't explore with value. The are both _max entropy_ action systems, who don't care about reward value or learning _at all_. I have kept the _ActorCritic_ style because it was easy to implement in _explorationlib_. Don't be misled."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = BanditActorCritic(\n",
    "    RandomActor(num_actions=env.num_arms),\n",
    "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    ")\n",
    "seq = BanditActorCritic(\n",
    "    SequentialActor(num_actions=env.num_arms),\n",
    "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    ")\n",
    "epy = BanditActorCritic(\n",
    "    EpsilonActor(num_actions=env.num_arms, epsilon=0.1),\n",
    "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [ran, seq, epy]\n",
    "names = [\"random\", \"sequential\", \"ep-greedy\"]\n",
    "colors = [\"blue\", \"green\", \"purple\"]"
   ]
  },
  {
   "source": [
    "Let's run out our three agents on the _env_ from our base, and plot some things about them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 12  # Three rounds per arm, about that anyway\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "source": [
    "#### Plot action choices \n",
    "with time (aka steps)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiment = 0\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    plot_bandit_actions(\n",
    "        select_exp(res, num_experiment), \n",
    "        num_arms=4,\n",
    "        s=4,\n",
    "        title=name, \n",
    "        color=color,\n",
    "        figsize=(2, 1.5)\n",
    "        )"
   ]
  },
  {
   "source": [
    "histograms action probability (aka arm choice). \n",
    "\n",
    "_Note_: The flatter these plots are, the closer they are to _maximum entropy_ exploration behavior. I added a measure of the actual entropy to the title of each plot, to make it easier to compare."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiment = 0\n",
    "ax = None\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    ent = np.round(np.mean(action_entropy(res)), 2)\n",
    "    plot_bandit_hist(\n",
    "        select_exp(res, num_experiment), \n",
    "        bins=list(range(0, 5)),\n",
    "        title=f\"{name} (ent {ent})\", \n",
    "        alpha=0.4,\n",
    "        color=color,\n",
    "        figsize=(3, 3),\n",
    "        ax=ax\n",
    "        )"
   ]
  },
  {
   "source": [
    "#### Question 1.1 \n",
    "How max entropy is max entropy sampling? If we ran the above 100 times, using different random seeds to the env and agents, would the measured entropy of random--who is max entropy asymptotically--ever exceed the sequential explorer?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's find out...."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for name, agent in zip(names[0:2], agents[0:2]):\n",
    "    tmp = []\n",
    "    for i in range(100):\n",
    "        env.seed(seed + i)\n",
    "        exp = experiment(\n",
    "            f\"{name}_{i}\",\n",
    "            agent,\n",
    "            env,\n",
    "            num_steps=num_steps,\n",
    "            num_experiments=1,\n",
    "            dump=False,\n",
    "            split_state=False,\n",
    "        )\n",
    "        ent = np.mean(action_entropy(exp))\n",
    "        tmp.append(ent)\n",
    "    scores.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3))\n",
    "for (name, s, c) in zip(names[0:2], scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, 2, 20))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Action entropy\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### Question 1.2\n",
    "Was your guess in Question 1.1 right? If it was, good job. If your guess was wrong, do you know why? Explain as best you can please."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.3\n",
    "Given what you have seen in Question 1.1, does random sampling seem more lesss like a good idea than sequential sampling. In other words, if you had to pick one be your own personal bandit explorer, which one would it be? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Meet our dilemma\n",
    "Should I explore? Should I exploit it!? OMG I DON'T KNOW, says space crab.\n",
    "\n",
    "....I'll flip a weight coin, \n",
    "\n",
    "...who's weight has a name. It's $\\epsilon$! \n",
    "\n",
    "The smaller $\\epsilon$ is, the less likely the coin flip comes up \"EXPLORE''. The more likely it comes up on the \"EXPLOIT\" side. If one chooses the exploit side, one is being greedy, right? The bigger $\\epsilon$ the more likely the coin will say \"EXPLORE ''. Etc.\n",
    "\n",
    "Let's play with $\\epsilon$-greedy, on our base case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 4 * 100\n",
    "epsilons = [0.05, 0.5, 0.95]\n",
    "\n",
    "names = [str(epsilon) for epsilon in epsilons]\n",
    "colors = [\"mediumpurple\", \"mediumorchid\", \"mediumvioletred\"]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for i, (name, epsilon) in enumerate(zip(names, epsilons)):\n",
    "    agent = BanditActorCritic(\n",
    "        EpsilonActor(num_actions=env.num_arms, epsilon=epsilon),\n",
    "        Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    "    )\n",
    "    log = experiment(\n",
    "        f\"ep_{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=100,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "source": [
    "Example behave. Change _num experiment_ to see more examples (0, 99). \n",
    "\n",
    "_Note_: in every experiment we run in this lab, the optimal value arm is _always_ arm 2."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiment = 40\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    plot_bandit_actions(\n",
    "        select_exp(res, num_experiment), \n",
    "        max_steps=200,\n",
    "        s=4,\n",
    "        title=name, \n",
    "        color=color,\n",
    "        figsize=(3, 1)\n",
    "        )"
   ]
  },
  {
   "source": [
    "Total reward "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.8)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.8, bins=np.linspace(1, 200, 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Total reward\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "Action entropy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = action_entropy(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.8, bins=np.linspace(0, 1.6, 40))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Entropy\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### Question 1.4\n",
    "In your own words explain how $\\epsilon$ seems to relate to _total reward_ and _action entropy_. As $\\epsilon$ grows, what happens?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.5\n",
    "Let's ask this again from another direction. What relationship do you see between _total reward_ and _action entropy_? To answer focus on the the two bar plots above, perhaps. \n",
    "\n",
    "Can you explain this correlation in terms of explore-exploit, and the bandit problem we are trying to understand/solve? Is it strong or weak? Can you say why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "## Section 2 - Bounds and bandits\n",
    "\n",
    "In this section we'll study two pure explorers with bounds. Our friend $\\epsilon$-greedy returns, unchanged. Our friend $\\epsilon$-greedy is a proxy for the question, \"Does the explore-exploit dilemma even matter, for bandits?\" We are going to give a (very very very) incomplete answer to this question by first tuning up our agents as best we can. Then, we put them in competition.\n",
    "\n",
    "#### Question 2.1\n",
    "Take a wild guess. Speculate. Use what little this lab has so far shown you. Does the dilemma matter? Will our friend $\\epsilon$-greedy emerge the winner?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "### Our (training) base case (Section 2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE:\n",
    "\n",
    "# Shared env params\n",
    "num_experiments = 100\n",
    "seed = 593  \n",
    "\n",
    "# Create env\n",
    "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
    "env.seed(seed)\n",
    "\n",
    "# Plot env\n",
    "plot_bandit(env, alpha=0.6)"
   ]
  },
  {
   "source": [
    "### A tune up\n",
    "Let's tune up each bandit, assuming 400 steps is an ok number."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 4 * 100\n",
    "num_experiments = 250"
   ]
  },
  {
   "source": [
    "#### Question 2.2 - Ep tuning.\n",
    "\n",
    "What is the best $\\epsilon$? (For this task).\n",
    "\n",
    "The code below is a first try at tuning _EpsilonActor_ and it's only parameter $\\epsilon$. Run it. Then use the code to try and narrow down a best choice for $\\epsilon$ in this environment. Put each attempt you make in a new cell, as a way to show your work. (Do not change _num steps_ or _num experiments_, above).\n",
    "\n",
    "Note: if you are having trouble, try plotting distributions instead of averages?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME\n",
    "start = 0.01 # min 0\n",
    "stop = 0.99 # max 1\n",
    "num_search = 10 # ?\n",
    "\n",
    "# -\n",
    "# LEAVE ME\n",
    "epsilons = np.linspace(start, stop, num_search)\n",
    "names = [str(np.round(epsilon, 2)) for epsilon in epsilons]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for i, (name, epsilon) in enumerate(zip(names, epsilons)):\n",
    "    agent = BanditActorCritic(\n",
    "        EpsilonActor(num_actions=env.num_arms, epsilon=epsilon),\n",
    "        Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    "    )\n",
    "    log = experiment(\n",
    "        f\"ep_{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)\n",
    "\n",
    "# Score\n",
    "scores = []\n",
    "for name, res in zip(names, results):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s) in zip(names, scores):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.8)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here; Find the best ep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are done, write the best ep you found here as a comment"
   ]
  },
  {
   "source": [
    "### Question 2.4 \n",
    "\n",
    "In the next question we'll tune the bound; the number of pure exploration steps before pure exploitation begins. But first, let's just the random pure explorer in action.\n",
    "\n",
    "Starting from 100, change the _bound_ below a few times, and plot the results-behave examples and reward distributions. Tell me what happens as the bound shrinks to 4 (aka one sample / arm, on average).\n",
    "\n",
    "_Note_: you can eyeball this, as I suggest, but it might be easier to write your own 'for loop' to see for sure?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = 100 # max 100\n",
    "ran = BanditActorCritic(\n",
    "    BoundedRandomActor(num_actions=env.num_arms, bound=bound),\n",
    "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    ")\n",
    "exp = experiment(\n",
    "        f\"example_bound\",\n",
    "        ran,\n",
    "        env,\n",
    "        num_steps=200,\n",
    "        num_experiments=100,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bandit_actions(\n",
    "        select_exp(exp, 0), \n",
    "        max_steps=200,\n",
    "        s=4,\n",
    "        title=bound, \n",
    "        color=\"black\",\n",
    "        figsize=(3, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.hist(total_reward(exp), label=bound, color=\"black\", alpha=0.8, bins=np.linspace(1, 200, 50))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Total reward\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 2.5\n",
    "What is the best _bound_ (For this task).\n",
    "\n",
    "The code below is a first try at tuning _BoundedRandomActor_ and its only parameter _bound_. Run it. Then use the code to try and narrrow down a best choice for $\\epsilon$ at this environment. Put each attempt you make in a new cell, as a way to show your work. (Do not change _num steps_ or _num experiments_, above).\n",
    "\n",
    "Note: if you are having trouble, try plotting distributions instead of averages?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME\n",
    "start = 4 # min 4\n",
    "stop = num_steps # max num_steps\n",
    "num_search = 10 # ?\n",
    "\n",
    "# -\n",
    "# LEAVE ME\n",
    "bounds = np.linspace(start, stop, num_search).astype(int)\n",
    "names = [str(np.round(bound, 2)) for bound in bounds]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for i, (name, bound) in enumerate(zip(names, bounds)):\n",
    "    agent = BanditActorCritic(\n",
    "        BoundedRandomActor(num_actions=env.num_arms, bound=bound),\n",
    "        Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    "    )\n",
    "    log = experiment(\n",
    "        f\"b_{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)\n",
    "\n",
    "# Score\n",
    "scores = []\n",
    "for name, res in zip(names, results):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s) in zip(names, scores):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.8)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.xlabel(\"Bound\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here; Find the best bound!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are done, write the best bound you found here as a comment"
   ]
  },
  {
   "source": [
    "#### Question 2.6\n",
    "Note: we are only going to tune the bound for _BoundedRandomActor_, and we'll rercycle this for _BoundedSequentialActor_. Can you guess why we are not optimizing both?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "### Our reporting base case (Section 2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE:\n",
    "\n",
    "# Shared env params\n",
    "seed = 195\n",
    "\n",
    "# Create env\n",
    "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
    "env.seed(seed)\n",
    "\n",
    "# Plot env\n",
    "plot_bandit(env, alpha=0.6)"
   ]
  },
  {
   "source": [
    "#### Question 2.7\n",
    "\n",
    "So, what's a better way to maximize total reward in this (reporting) task? Is it BoundedRandomActor? Or, BoundedSeqentialActor? Or is it _EpsilonActor_?\n",
    "\n",
    "To answer, use the optimized parameters from Q2.2-6 to run a series of experiments, with a variable number of steps. Report the total rewards collected for each of the three agents for the num_steps below. I am not going to do any work for you this time. You're on your own for the code!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = [4, 8, 16, 40, 80, 200, 400, 1000, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here; Find the best agent for each num_steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are done, write the best agent here"
   ]
  },
  {
   "source": [
    "#### Question 2.8 - the big conclusion!\n",
    "\n",
    "Based on what you've seen here today, do you think pure exploration is a good way to solve reward learning problems? Assume when you answer that our 4-armed bandit is a good proxy for the real world. (...This is a bad asssumption... deal.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  }
 ]
}