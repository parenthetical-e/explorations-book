{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('py3.6': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5c0fa7a4f8f1487a2aac67eb43e7b2e553808a81f9be50af9e1ab194481cfe22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# The Air Quotes Cognition - Lab\n",
    "\n",
    "In this assignment we take on noise, aka uncertainty, in exploration. Our venue is still chemotaxis. But now our sensors are noisy, or the world is \"turbulent\", or some other thing is happening to cause us doubt our senses. The presence of this uncertainty makes decisions--of the kind common to decision theory--a necessity. \n",
    "\n",
    "The decision to be made is this: is the gradient increasing or decreasing? \n",
    "\n",
    "We also introduce several targets.\n",
    "\n",
    "We'll compare two exploring agents. The gradient searcher from _Sniff! Lab_ who, you may remember, operates akin to a _E. Coli_. A really simple model of an _E. Coli_, anyway. We'll also add in a new kind of gradient searcher, who uses a DDM-style accumulator to try and make better decisions about the direction of the gradient. These decisions are of course statistical in nature.\n",
    "\n",
    "There are two sections. First we examine the accumulator. Second we compare it to our other agent, our naive sniffer. Here, in this section, there is a twist! The big idea for this twist I will spoil now. It is a question. Is it better to spend time rationally accumulating evidence, or is it better to just act? \n",
    "\n",
    "We'll play around with this big question in a simple way, by setting up something like a conservation law of \"ticks\" (aka steps). Steps can be spent thinking _or_ acting. I assume in this an agent can't think and act at the same time.\n",
    "\n",
    "A bit of warning: in previous labs we have used only one metric. Either search efficiency, or total reward. In this lab, to really understand the thinking-action trade-off, we'll be looking at the results from a few more angles. These are:\n",
    "- average reward \n",
    "- best reward\n",
    "- total distance travelled\n",
    "- number of deaths*\n",
    "\n",
    "Any experimental trial which does not lead to finding at least a single target (aka reward) means the exploring agent dies. It's a harsh noisy world we live in, after all. \n",
    "\n",
    "## Install and import needed modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install explorationlib?\n",
    "!pip install --upgrade git+https://github.com/parenthetical-e/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import misc\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "# Vis - 1\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exp\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "# Agents\n",
    "from explorationlib.agent import GradientDiffusionGrid\n",
    "from explorationlib.agent import AccumulatorGradientGrid\n",
    "\n",
    "# Env\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "from explorationlib.local_gym import create_grid_scent\n",
    "from explorationlib.local_gym import uniform_targets\n",
    "from explorationlib.local_gym import constant_values\n",
    "\n",
    "# Vis - 2\n",
    "from explorationlib.plot import plot_position2d\n",
    "from explorationlib.plot import plot_length_hist\n",
    "from explorationlib.plot import plot_length\n",
    "from explorationlib.plot import plot_targets2d\n",
    "from explorationlib.plot import plot_scent_grid\n",
    "\n",
    "# Score\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import num_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.greedy=True\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = \"16\"\n",
    "\n",
    "# Dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "## Section 1 - patience, and the weight of evidence\n",
    "\n",
    "In this section we'll start with a noisy, somewhat target-filled, domain and explore how speed, accuracy, and \n",
    "\n",
    "_Background_: Recall the model of scent in our (naive) _Sniff!_ agent (aka _GradientDiffusionDiscrete_) is as simple as can be. \n",
    "\n",
    "- When the scent gradient is positive, meaning you are going \"up\" the gradient, the probability of turning is set to _p pos_. \n",
    "- When the gradient is negative, the turning probability is set to _p neg_. (See code below, for an example). \n",
    "- If the agent \"decides\" to turn, the direction is uniform random.\n",
    "- The length of travel before the next turn decision is sampled from an exponential distribution just like the _DiffusionDiscrete_\n",
    "\n",
    "This same design holds for the accumulating sniffer, but it also can spend time sampling evidence in the same location. This should let us make better gradient decisions, in the very noisy world I will define anyway."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this section we are going to examine how the _drift rate_ and the decision _threshold_ affect, best outcomes, average outcomes, and number of deaths. We'll examine these two parameters separately. Holding one or the other constant.\n",
    "\n",
    "### Question 1.1\n",
    "\n",
    "Based on what you have been told so far, how would you expect increases in _drift rate_ (aka the evidence rate) to effect average rewards, best rewards, and deaths in an open field task, with sparse targets and noisy scents?\n",
    "\n",
    "Make your best guess, and explain yourself."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment"
   ]
  },
  {
   "source": [
    "### Question 1.2\n",
    "\n",
    "Based on what you have been told so far, how would you expect increases in the decision _threshold_ (aka patience or \"cognition\" time) to effect average rewards, best rewards, and deaths in an open field task, with sparse targets and noisy scents.\n",
    "\n",
    "Make your best guess, and explain yourself."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "The name of the env for this section is once again the _ScentGrid_. \n",
    "\n",
    "Let's add some targets and scents to it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared exp parameters\n",
    "num_experiments = 200\n",
    "num_steps = 200\n",
    "max_steps = 10\n",
    "seed_value = 5838\n",
    "\n",
    "min_length = 1\n",
    "step_size = 0.1\n",
    "\n",
    "noise_sigma = 2\n",
    "detection_radius = 1\n",
    "num_targets = 250 \n",
    "target_boundary = (100, 100)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "coord, scent = create_grid_scent(target_boundary, amplitude=1, sigma=10)\n",
    "scents = [scent for _ in range(len(targets))]\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"
   ]
  },
  {
   "source": [
    "Using the _env_ defined above explore the following drift rates\n",
    "### Drift rates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our parameters \n",
    "drift_rates = [0.25, 0.75, 1.0, 1.25, 1.5]\n",
    "\n",
    "# For plotting\n",
    "colors = [\"darkgreen\", \"seagreen\", \"cadetblue\", \"steelblue\", \"mediumpurple\"]\n",
    "names = list(range(5))"
   ]
  },
  {
   "source": [
    "### Run\n",
    "200 experiments for each drift rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp params\n",
    "threshold = 3.0\n",
    "accumulate_sigma = 1.0\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for i, drift_rate in zip(names, drift_rates):\n",
    "    accum = AccumulatorGradientGrid(\n",
    "        min_length=min_length, \n",
    "        max_steps=max_steps, \n",
    "        drift_rate=drift_rate, \n",
    "        threshold=threshold,\n",
    "        accumulate_sigma=accumulate_sigma\n",
    "    )\n",
    "    accum.seed(seed_value)\n",
    "    # !\n",
    "    exp = experiment(\n",
    "        f\"accum_{i}\",\n",
    "        accum,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=True,\n",
    "        seed=seed_value\n",
    "    )\n",
    "    results.append(exp)"
   ]
  },
  {
   "source": [
    "#### Plot an example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary = (50, 50)\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "for i, result, color in zip(names, results, colors):\n",
    "    ax = plot_position2d(\n",
    "        select_exp(result, num_experiment),\n",
    "        boundary=plot_boundary,\n",
    "        label=i,\n",
    "        color=color,\n",
    "        alpha=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "source": [
    "#### Plot several metrics\n",
    "Distance, death, best reward, average reward\n",
    "\n",
    "Note: the model code follows the _drift rate_, See the def. of \"names\" above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:  \n",
    "    l = 0.0\n",
    "    for r in result:\n",
    "        l += r[\"agent_total_l\"][-1]\n",
    "    scores.append(l)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.mean(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Total distance\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    scores.append(num_death(result))   \n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], scores, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m = []\n",
    "for s in scores:\n",
    "    m.append(np.max(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:  \n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Avg. score\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists of means\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (i, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=i, color=c, alpha=0.5, bins=list(range(1,50,1)))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "### Question 1.3\n",
    "Based on the plots above, in your own words summarize the relationship between _drift rate_ and total distance, number of deaths, best reward and average reward."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "### Question 1.4\n",
    "Recall that the larger the drift rate, the faster the agent will reach a decision. Based on the above do you think it's better to be faster to choose, or more accurate in this task?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "### Thresholds\n",
    "Explore five thresholds, in the same environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our parameters \n",
    "thresholds = [0.1, 1.0, 2.0, 3.0, 4.0]\n",
    "# For plotting\n",
    "colors = [\"darkgreen\", \"seagreen\", \"cadetblue\", \"steelblue\", \"mediumpurple\"]\n",
    "names = list(range(5))"
   ]
  },
  {
   "source": [
    "### Run \n",
    "200 experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -\n",
    "results = []\n",
    "for i, threshold in zip(names, thresholds):\n",
    "    accum = AccumulatorGradientGrid(\n",
    "        min_length=min_length, \n",
    "        max_steps=max_steps, \n",
    "        drift_rate=drift_rate, \n",
    "        threshold=threshold,\n",
    "        accumulate_sigma=accumulate_sigma\n",
    "    )\n",
    "    accum.seed(seed_value)\n",
    "    # !\n",
    "    exp = experiment(\n",
    "        f\"accum_{i}\",\n",
    "        accum,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=True,\n",
    "        seed=seed_value\n",
    "    )\n",
    "    results.append(exp)"
   ]
  },
  {
   "source": [
    "#### Plot an example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary = (50, 50)\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "for i, result, color in zip(names, results, colors):\n",
    "    ax = plot_position2d(\n",
    "        select_exp(result, num_experiment),\n",
    "        boundary=plot_boundary,\n",
    "        label=i,\n",
    "        color=color,\n",
    "        alpha=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "source": [
    "#### Plot several metrics\n",
    "Distance, death, best reward, average reward\n",
    "\n",
    "Note: the model code follows the _drift rate_, See the def. of \"names\" above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    l = 0.0\n",
    "    for r in result:\n",
    "        l += r[\"agent_total_l\"][-1]\n",
    "    scores.append(l)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in zip(scores):\n",
    "    m.append(np.mean(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Total distance\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    scores.append(num_death(result))   \n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], scores, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results: \n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m = []\n",
    "for s in scores:\n",
    "    m.append(np.max(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Avg. score\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (i, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=i, color=c, alpha=0.5, bins=list(range(1,50,1)))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "### Question 1.5\n",
    "Based on the plots above and in your own words, summarize the relationship between the decision _threshold_ and the total distance, number of deaths, best reward and average reward."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "### Question 1.6\n",
    "Make a prediction. Based on your observations and answers to all the questions in this section, do you think an accumulator is needed? That is, will it perform better, or worse, than the naive sniffer in this task?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "### Question 1.7\n",
    "Let's get philosophical. Is it better to gamble on being the best, knowing you risk death (in this task) or is it better to be average and alive? \n",
    "\n",
    "What parameters from the above do you _personally_ prefer. There is no right answer. Be yourself."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "## Section 2 - the evidence is in?\n",
    "\n",
    "In this section we'll start in the same noisy, somewhat target-filled, domain as Section 1. This time we'll compare the naive sniffer, to an accumulator model. I have chosen parameters for each to be good enough. (No need to fiddle).\n",
    "\n",
    "The parameter of interest here is the number of steps, _num steps_, each agent can take. \n",
    "\n",
    "In previous labs the number of steps meant the number of steps or actions the agent took in the environment. Now, here, it can mean two things for the accumulator agent. A step can be spent sampling/weighing noisy scent evidence in the same location, or it can be spent moving to a new location. \n",
    "\n",
    "### Question 2.1\n",
    "For the same number of steps (200), make a brute guess. Which will do better, the naive sniffer or the accumulator? \n",
    "\n",
    "In your answer assume I have given you good parameter choices, so all things are held equal, more or less. I believe I have done so."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "### Run \n",
    "\n",
    "Let's find out if you were right"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment settings\n",
    "num_experiments = 200\n",
    "num_steps = 200\n",
    "max_steps = 10\n",
    "seed_value = 42\n",
    "\n",
    "# Fixed\n",
    "min_length = 1\n",
    "step_size = 0.1\n",
    "\n",
    "noise_sigma = 2\n",
    "detection_radius = 1\n",
    "num_targets = 250 # 5000-50000\n",
    "target_boundary = (100, 100)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "coord, scent = create_grid_scent(target_boundary, amplitude=1, sigma=10)\n",
    "scents = [scent for _ in range(len(targets))]\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents\n",
    "sniff = GradientDiffusionGrid(\n",
    "    min_length=min_length, \n",
    "    scale=1.0, \n",
    "    p_neg=1, \n",
    "    p_pos=0.0\n",
    ")\n",
    "sniff.seed(seed_value)\n",
    "\n",
    "accum = AccumulatorGradientGrid(\n",
    "    min_length=min_length, \n",
    "    max_steps=max_steps, \n",
    "    drift_rate=1, \n",
    "    threshold=3,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "accum.seed(seed_value)\n",
    "\n",
    "# !\n",
    "accum_exp = experiment(\n",
    "    f\"accum\",\n",
    "    accum,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "sniff_exp = experiment(\n",
    "    f\"sniff\",\n",
    "    sniff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join \n",
    "results = [accum_exp, sniff_exp]\n",
    "names = [\"Accum\", \"Sniff\"]\n",
    "colors = [\"blue\", \"green\"]"
   ]
  },
  {
   "source": [
    "#### Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary = (50, 50)\n",
    "\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(accum_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=\"Accum\",\n",
    "    color=\"blue\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_position2d(\n",
    "    select_exp(sniff_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=\"Sniff\",\n",
    "    color=\"green\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "source": [
    "#### Plot several metrics\n",
    "Distance, death, best reward, average reward\n",
    "\n",
    "Note: the model code follows the _drift rate_, See the def. of \"names\" above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    l = 0.0\n",
    "    for r in result:\n",
    "        l += r[\"agent_total_l\"][-1]\n",
    "    scores.append(l)   \n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, scores, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Total distance\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    scores.append(num_death(result))   \n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, scores, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:  \n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m = []\n",
    "for s in zip(scores):\n",
    "    m.append(np.max(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:  \n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.max(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:  \n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Avg. score\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (i, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=i, color=c, alpha=0.5, bins=list(range(1,50,1)))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "### Question 2.2\n",
    "\n",
    "Was your answer to Q 2.1 correct? \n",
    "\n",
    "If it was right, tell me how smart you are, and how bright your brillant star shines, in a 5-7-5 Haiku. If you were wrong, tell me why you think that was. Also, do this in the form of a 5-7-5 Haiku."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's try and give the accumulator some more time to do its work, but keep the steps the naive sniffer has the same. \n",
    "\n",
    "Let's call this the _cognitive multiplier_ (cog mult). To keep things sane, let's cap the values of the cog mult between 1 and 10. \n",
    "\n",
    "### Question 2.3\n",
    "\n",
    "In the given range how much past 1 does the _cog mult_ need to be set to ensure the number of deaths is the same between the models? Make a guess. if you can, explain why."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. "
   ]
  },
  {
   "source": [
    "### Run\n",
    "Let's find out if you were right.\n",
    "\n",
    "Use the code below to find a _cog mult_ value (1-10) that has about equal number of deaths for each of the agents. No need to be exact. Get them close."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment settings\n",
    "cog_mult = 1\n",
    "\n",
    "# Agents\n",
    "sniff = GradientDiffusionGrid(\n",
    "    min_length=min_length, \n",
    "    scale=1.0, \n",
    "    p_neg=1, \n",
    "    p_pos=0.0\n",
    ")\n",
    "sniff.seed(seed_value)\n",
    "accum = AccumulatorGradientGrid(\n",
    "    min_length=min_length, \n",
    "    max_steps=max_steps, \n",
    "    drift_rate=1, \n",
    "    threshold=3,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "accum.seed(seed_value)\n",
    "\n",
    "# !\n",
    "accum_exp = experiment(\n",
    "    f\"data/test_accum.pkl\",\n",
    "    accum,\n",
    "    env,\n",
    "    num_steps=cog_mult*num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "sniff_exp = experiment(\n",
    "    f\"data/test_sniff.pkl\",\n",
    "    sniff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join \n",
    "results = [accum_exp, sniff_exp]\n",
    "names = [\"Accum\", \"Sniff\"]\n",
    "colors = [\"blue\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    scores.append(num_death(result))   \n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, scores, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "source": [
    "### Question 2.4\n",
    "\n",
    "What is the final value of _cog mult_ you chose?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. "
   ]
  },
  {
   "source": [
    "### Question 2.4\n",
    "\n",
    "Do you think that equalizing deaths will cause the accumulator to outperform the naive sniffer in distance travelled, max reward, and average reward? Explain your answer for each metric."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself"
   ]
  },
  {
   "source": [
    "### Run \n",
    "\n",
    "Let's find out if you were right. Run experiment code immediately above, with your final value of _cog mult_. Once done, come back here and run the cells below to plot the results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    l = 0.0\n",
    "    for r in result:\n",
    "        l += r[\"agent_total_l\"][-1]\n",
    "    scores.append(l)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.mean(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Total distance\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    scores.append(num_death(result))   \n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], scores, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.max(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for i, result, color in zip(names, results, colors):    \n",
    "    r = total_reward(result)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in zip(scores):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar([str(n) for n in names], m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Avg. score\")\n",
    "plt.xlabel(\"Model code\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (i, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=i, color=c, alpha=0.5, bins=list(range(1,50,1)))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "### Question 2.5\n",
    "\n",
    "In light of all this, when is it better to weigh evidence, in an optimal way, as our accumulator does, and when it is better to just act? Phrase your answer in terms of the number of \"extra\" steps an accumulator needs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself"
   ]
  },
  {
   "source": [
    "### Question 2.6\n",
    "\n",
    "Perhaps animals need a minimal time, or minimal resources, for cognition to pay out!? Do you think this simple lab would let us reach such a broad conclusion as this last question suggests? Let's assume not! Provide please a counterexample - a change to the task which would ensure evidence accumulation is _always_ worthwhile. _Or_, a change to make it never worthwhile. Either or, that is."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}