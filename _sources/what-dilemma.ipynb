{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# The What Dilemma - Lab\n",
    "## Introduction\n",
    "In this assignment we take on all the things, err, agents, that we have ever studied. And two new ones, to boot! \n",
    "\n",
    "This is the final lab. \n",
    "\n",
    "Space crab is a sad crab....\n",
    "\n",
    "_The decisions to be made this week are the exact opposite of every other lab_. \n",
    "\n",
    "I am giving you six tuned agents, and three \"levers\" which control the environment. The now familiar scent grid. Your job this week is to tweak the environment, until each of the agents is the winning agents, in two senses.\n",
    "\n",
    "Our target metrics:\n",
    "- It must gather the most total reward, by a clear margin (error baar overlap)\n",
    "- It must not die the most. That is, as long as one other agent dies more often, or all agents die 0 times, we'll call that good enough. (Any experimental trial which does not lead to finding at least a single target (aka reward) means the exploring agent dies. It's a harsh noisy world we live in, after all.)\n",
    "\n",
    "Once again, on final time it's time for _taxic explorations_. We revisit the sniff world (aka _ScentGrid_) with a familiar twist. We look again at what happens when sense information is not just noisy, but suddenly missing altogether. A concrete, cheap to simulate, case of this is turbulent flows. \n",
    "\n",
    "### Sections\n",
    "There are two sections to this Lab. In the first we get to know WSLS, as well as a pure RL agent. In the second, we explore and change the environment itself. \n",
    "\n",
    "### The env levers\n",
    "There are three \"levers'' you may put to use:\n",
    "\n",
    "- num_targets = (1, 1000)  # these are the allowed bounds\n",
    "- noise_sigma = (0.0, 10)\n",
    "- cog_mult = (1, 10)\n",
    "\n",
    "### Our agents, this time\n",
    "We will study six agents. They are,\n",
    "\n",
    "- A diffusion walker (aka rando-taxis) (aka _DiffusionGrid_)\n",
    "- Sniff! (aka chemo-taxis) (aka _GradientDiffusionGrid_)\n",
    "- Air cognition! (aka \"smart\" chemo-taxis) (aka _AccumulatorGradientGrid_)\n",
    "- Info cognition! (aka \"smart\" info-taxis) (aka _AccumulatorInfoGrid_)\n",
    "- RL w/ random softmax search (aka _ActorCriticGrid_)\n",
    "- Curiosity and RL union (aka rewardo- and info-taxis) (aka _WSLSGrid_)\n",
    "\n",
    "The goal is, as I said, to _the change the world_ -- until each agent \"wins\" (defined above).\n",
    "\n",
    "### Our agents, in review\n",
    "\n",
    "**Random search** (rando-taxis): Actions are sampled from an exponential distribution. For the _randotaxis_ agent number of steps means the number of steps or actions the agent takes. \n",
    "\n",
    "**Sniff!** (chemo-taxis): Recall our basic model of E. Coli exploration is as simple as can be. \n",
    "\n",
    "- When the gradient is positive, meaning you are going \"up\" the gradient, the probability of turning is set to _p pos_. \n",
    "- When the gradient is negative, the turning probability is set to _p neg_. (See code below, for an example). \n",
    "- If the agent \"decides\" to turn, the direction it takes is uniform random.\n",
    "- The length of travel before the next turn decision is sampled from an exponential distribution just like the _DiffusionGrid_\n",
    "\n",
    "**Costly cognition** (\"smart\", chemo- and info-taxis): Both _chemo-_ and _infotaxis_ agents will use a DDM-style accumulator to try and make better decisions about the direction of the gradient. These decisions are of course statistical in nature. (We won't be tuning the accumulator parameters in this lab. Assume the parameters I give you, for the DDM, are \"good enough\".)\n",
    "\n",
    "As in the _Air Quotes Lab_ we will assume that the steps are in a sense conserved. For the other two (accumulator) agents a step can mean two things. For accumulator agents a step can be spent sampling/weighing noisy scent evidence in the same location, or it can be spent moving to a new location. _Note_: Even though the info-accumulator is more complex, it can take advantage of missing scent information to drive its behavior. It can also use positive scent hits, of course, too.\n",
    "\n",
    "**RL** (rewardo-taxis): A Q-learning agent with softmax exploration. Recall: this is the same kind of agent we studied in the Cliff task, in the _The Oh No! - Lab_.\n",
    "\n",
    "The RL agent has no shaping function, or intrinsic reward. It does not use the scent, in other words.\n",
    "\n",
    "**WSLS** (rewardo- and info-taxis): A agent that alternates between info-taxis and Q-learning. Both are deterministic. Exploration and exploitation without any random search, in other words. \n",
    "\n",
    "_Details_: For this model a memory $M$ is a discrete probability distribution. I define information value $E$ on the norm of the derivative ($\\nabla M), approximated by $\\hat E = || f(x, M) - M ||$, where $||.||$ denotes the norm. (Norms are distances like hypotanooses.) \n",
    "\n",
    "The goal of any info-taxis (aka, curiosity agent) is to maximize $E$, I claim, based on a Bellman-optimal policy $\\pi^*_E$. \n",
    "\n",
    "So armed with $\\hat E$ I write down another (meta) policy $\\pi^{\\pi}$, in terms of a mixed series of values, $\\hat E$ and environmental rewards $R$. This WSLS rule is shown below. The reward (exploit) policy $\\pi_R$ is Q learning, same as for **RL**.\n",
    "\n",
    "$$\n",
    "    \\begin{split}\n",
    "        \\Pi_{\\pi} = \n",
    "        \\begin{cases}\n",
    "            \\pi^*_{\\hat{E}} & : \\hat{E} - \\eta > R + \\rho \\\\\n",
    "            \\pi_R \t& : \\hat{E} - \\eta < R + \\rho \\\\\n",
    "        \\end{cases}\n",
    "    \\end{split}\n",
    "$$\n",
    "\n",
    "## Our TED talk moment \n",
    "\n",
    "AKA, _Your moment in the sun!_\n",
    "\n",
    "AAKA, _Nine new Powers are born!_\n",
    "\n",
    "AAAKA, _Change the world, my students!_\n",
    "\n",
    "Let each species (agent) know the sweet comfort of utter ecological dominance. ...or try to... **I do not promise victory is always possible.**\n",
    "\n",
    "## Install and import needed modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install explorationlib?\n",
    "!pip install --upgrade git+https://github.com/parenthetical-e/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "import explorationlib\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "\n",
    "from explorationlib.agent import WSLSGrid\n",
    "from explorationlib.agent import CriticGrid\n",
    "from explorationlib.agent import SoftmaxActor\n",
    "from explorationlib.agent import DiffusionGrid\n",
    "from explorationlib.agent import GradientDiffusionGrid\n",
    "from explorationlib.agent import AccumulatorGradientGrid\n",
    "from explorationlib.agent import AccumulatorInfoGrid\n",
    "from explorationlib.agent import ActorCriticGrid\n",
    "\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "from explorationlib.local_gym import uniform_targets\n",
    "from explorationlib.local_gym import constant_values\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "from explorationlib.local_gym import create_grid_scent\n",
    "from explorationlib.local_gym import add_noise\n",
    "from explorationlib.local_gym import create_grid_scent_patches\n",
    "\n",
    "from explorationlib.plot import plot_position2d\n",
    "from explorationlib.plot import plot_length_hist\n",
    "from explorationlib.plot import plot_length\n",
    "from explorationlib.plot import plot_targets2d\n",
    "from explorationlib.plot import plot_scent_grid\n",
    "from explorationlib.plot import plot_targets2d\n",
    "\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import num_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.greedy=True\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = \"16\"\n",
    "\n",
    "# Dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "## Section 1 -  RL and WSLS\n",
    "### RL\n",
    "\n",
    "To build some intuition, let's plot the the behavoir of our RL agent as it learns where the rewards are in a (fixed) ScentGrid env. The noise level is 2 standard deviaions, all but 10 percent of it deleted. \n",
    "\n",
    "\n",
    "#### Question 1.1\n",
    "Does the fact that \n",
    "\n",
    "> The noise level of the scents is 2 standard deviaions, and all but 10 percent of it deleted. \n",
    "\n",
    "matter for the RL agent?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "### Shared params and env\n",
    "Section 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise and delete\n",
    "p_scent = 0.1\n",
    "noise_sigma = 2.0\n",
    "\n",
    "# Shared \n",
    "num_experiments = 100\n",
    "num_steps = 200\n",
    "seed_value = 5838\n",
    "num_targets = 20 # with 80 agents are more competitive!\n",
    "\n",
    "# ! (leave alone)\n",
    "detection_radius = 1\n",
    "cog_mult = 1\n",
    "max_steps = 1\n",
    "min_length = 1\n",
    "target_boundary = (10, 10)\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "scents = []\n",
    "for _ in range(len(targets)):\n",
    "    coord, scent = create_grid_scent_patches(\n",
    "        target_boundary, p=1.0, amplitude=1, sigma=2)\n",
    "    scents.append(scent)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"
   ]
  },
  {
   "source": [
    "### Getting to know you, RL\n",
    "\n",
    "...and a random walker reference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL\n",
    "possible_actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "critic = CriticGrid(default_value=0.5)\n",
    "actor = SoftmaxActor(num_actions=4, actions=possible_actions, beta=4)\n",
    "rl = ActorCriticGrid(actor, critic, lr=0.1, gamma=0.1)\n",
    "\n",
    "# Rando\n",
    "diff = DiffusionGrid(min_length=min_length, scale=1)\n",
    "diff.seed(seed_value)\n",
    "\n",
    "# !\n",
    "rl_exp = experiment(\n",
    "    f\"RL\",\n",
    "    rl,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "source": [
    "### Rando search\n",
    "Just one example, for comparison with the cells below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 99\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(rand_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"Rando\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "source": [
    "### Search behavoir, and learning\n",
    "At three experimental time points, $N$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=0.3,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 50\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 99\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=1,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "source": [
    "### Reward value, in time\n",
    "At three experimental time points, $N$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.plot(rl_exp[0][\"agent_reward_value\"], label=\"N=0\", color=\"orange\", alpha=0.2)\n",
    "plt.plot(rl_exp[50][\"agent_reward_value\"], label=\"N=50\", color=\"orange\", alpha=0.5)\n",
    "plt.plot(rl_exp[99][\"agent_reward_value\"], label=\"N=99\", color=\"orange\", alpha=1)\n",
    "plt.ylabel(\"Value $V(x)$\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "source": [
    "### Death"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp]\n",
    "names = [\"Rando\", \"RL\"]\n",
    "colors = [\"grey\", \"orange\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "source": [
    "### Total reward"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp]\n",
    "names = [\"Rando\", \"RL\"]\n",
    "colors = [\"grey\", \"orange\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "### Is it really better to WSLS?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WSLS\n",
    "possible_actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "num_action = len(possible_actions)\n",
    "initial_bins = np.linspace(0, 1, 10)\n",
    "\n",
    "critic_R = CriticGrid(default_value=0.0)\n",
    "critic_E = CriticGrid(default_value=np.log(num_action))\n",
    "actor_R = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20) \n",
    "actor_E = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20)\n",
    "\n",
    "wsls = WSLSGrid(\n",
    "    actor_E,\n",
    "    critic_E,\n",
    "    actor_R,\n",
    "    critic_R,\n",
    "    initial_bins,\n",
    "    lr=0.1,\n",
    "    gamma=0.1,\n",
    "    boredom=0.0\n",
    ")\n",
    "\n",
    "# Rando\n",
    "diff = DiffusionGrid(min_length=min_length, scale=1)\n",
    "diff.seed(seed_value)\n",
    "\n",
    "# !\n",
    "wsls_exp = experiment(\n",
    "    f\"wsls\",\n",
    "    wsls,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 99\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(rand_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"Rando\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(wsls_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orangered\",\n",
    "    alpha=0.3,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 50\n",
    "ax = plot_position2d(\n",
    "    select_exp(wsls_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orangered\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 99\n",
    "ax = plot_position2d(\n",
    "    select_exp(wsls_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orangered\",\n",
    "    alpha=1,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "source": [
    "### Reward value, in time\n",
    "At three experimental time points, $N$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.plot(wsls_exp[0][\"agent_reward_value\"], label=\"N=0\", color=\"orangered\", alpha=0.2)\n",
    "plt.plot(wsls_exp[50][\"agent_reward_value\"], label=\"N=50\", color=\"orangered\", alpha=0.5)\n",
    "plt.plot(wsls_exp[99][\"agent_reward_value\"], label=\"N=99\", color=\"orangered\", alpha=1)\n",
    "plt.ylabel(\"Value $V(x)$\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "source": [
    "### Death"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, wsls_exp]\n",
    "names = [\"Rando\", \"WSLS\"]\n",
    "colors = [\"grey\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.9)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "source": [
    "### Total reward"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, wsls_exp]\n",
    "names = [\"Rando\", \"WSLS\"]\n",
    "colors = [\"grey\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "### So, is it better to be curious and greedy, or greedy and noisy?\n",
    "A comparison between RL and WSLS (and rando)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Death"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp, wsls_exp]\n",
    "names = [\"Rando\", \"RL\", \"WSLS\"]\n",
    "colors = [\"grey\", \"orange\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.9)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "source": [
    "### Total reward"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp, wsls_exp]\n",
    "names = [\"Rando\", \"RL\", \"WSLS\"]\n",
    "colors = [\"grey\", \"orange\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "#### Question 1.2\n",
    "The WSLS approach should have generated more total reward. It may also have had a few (< 10) deaths. (If it did not, try running the WSLS cells again).\n",
    "\n",
    "Likewise, if you study WSLS search behavior and value learning time courses, you'll see it \"settles down\" to one rewarding spot and can stay there.\n",
    "\n",
    "In other words, WSLS is a method with very high inductive bias.\n",
    "\n",
    "A theme of this class has been, “bias is great... until it is not”.\n",
    "\n",
    "Based on the results in this lab so far, and lecture on WSLS, how could you change the env so that the exploration bias behind WSLS (deterministic learning maximization) fails, but the random search of RL does not.\n",
    "\n",
    "_Note:_ It is helpful to consider the total reward distribution plots carefully. The middle and the bottom range, especially. (Try rerunning?)\n",
    "\n",
    "_Note_: Everything is on the table. Your counter-example can be whatever you want, well as long as it is physically possible. Be imaginative!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "## Section 2\n",
    "\n",
    "Let's remake the world....\n",
    "\n",
    "### All our agents\n",
    "Run on the same world from Section 1. An example to see where things stand. To give you a place to start in your world building."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Intial (reference) env\n",
    "Section 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise and delete\n",
    "p_scent = 0.1\n",
    "noise_sigma = 2.0\n",
    "\n",
    "# Shared \n",
    "num_experiments = 100\n",
    "num_steps = 200\n",
    "seed_value = 5838\n",
    "num_targets = 20 # with 80 agents are more competitive!\n",
    "\n",
    "# ! (leave alone)\n",
    "detection_radius = 1\n",
    "cog_mult = 1\n",
    "max_steps = 1\n",
    "min_length = 1\n",
    "target_boundary = (10, 10)\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "scents = []\n",
    "for _ in range(len(targets)):\n",
    "    coord, scent = create_grid_scent_patches(\n",
    "        target_boundary, p=1.0, amplitude=1, sigma=2)\n",
    "    scents.append(scent)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"
   ]
  },
  {
   "source": [
    "### Run 'em all!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents\n",
    "\n",
    "# rando\n",
    "diff = DiffusionGrid(min_length=min_length, scale=1)\n",
    "diff.seed(seed_value)\n",
    "\n",
    "# sniff\n",
    "sniff = GradientDiffusionGrid(\n",
    "    min_length=min_length, \n",
    "    scale=1.0, \n",
    "    p_neg=1, \n",
    "    p_pos=0.0\n",
    ")\n",
    "sniff.seed(seed_value)\n",
    "\n",
    "# smart chemo\n",
    "chemo = AccumulatorGradientGrid(\n",
    "    min_length=min_length, \n",
    "    max_steps=max_steps, \n",
    "    drift_rate=1, \n",
    "    threshold=3,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "chemo.seed(seed_value)\n",
    "\n",
    "# smart info\n",
    "info = AccumulatorInfoGrid(\n",
    "    min_length=min_length, \n",
    "    max_steps=max_steps, \n",
    "    drift_rate=1, \n",
    "    threshold=3,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "info.seed(seed_value)\n",
    "\n",
    "# RL\n",
    "critic = CriticGrid(default_value=0.5)\n",
    "actor = SoftmaxActor(num_actions=4, actions=possible_actions, beta=4)\n",
    "rl = ActorCriticGrid(actor, critic, lr=0.1, gamma=0.1)\n",
    "\n",
    "# WSLS\n",
    "possible_actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "num_action = len(possible_actions)\n",
    "initial_bins = np.linspace(0, 1, 10)\n",
    "\n",
    "critic_R = CriticGrid(default_value=0.5)\n",
    "critic_E = CriticGrid(default_value=np.log(num_action))\n",
    "actor_R = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20) \n",
    "actor_E = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20)\n",
    "\n",
    "wsls = WSLSGrid(\n",
    "    actor_E,\n",
    "    critic_E,\n",
    "    actor_R,\n",
    "    critic_R,\n",
    "    initial_bins,\n",
    "    lr=0.1,\n",
    "    gamma=0.1,\n",
    "    boredom=0.0\n",
    ")\n",
    "\n",
    "# !\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "sniff_exp = experiment(\n",
    "    f\"sniff\",\n",
    "    sniff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "chemo_exp = experiment(\n",
    "    f\"chemo\",\n",
    "    chemo,\n",
    "    env,\n",
    "    num_steps=num_steps * cog_mult,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "info_exp = experiment(\n",
    "    f\"info\",\n",
    "    info,\n",
    "    env,\n",
    "    num_steps=num_steps * cog_mult,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "rl_exp = experiment(\n",
    "    f\"rl\",\n",
    "    rl,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "wsls_exp = experiment(\n",
    "    f\"wsls\",\n",
    "    wsls,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "source": [
    "### Search behavoir \n",
    "- Experiment 99"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "num_experiment = 99\n",
    "\n",
    "# Results\n",
    "results = [sniff_exp, chemo_exp, info_exp, rand_exp, rl_exp, wsls_exp]\n",
    "names = [\"Sniff\", \"Chemo\", \"Info\", \"Rando\", \"RL\", \"WSLS\"]\n",
    "colors = [\"purple\", \"blue\", \"green\", \"grey\", \"orange\", \"orangered\"]\n",
    "\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    ax = None\n",
    "    ax = plot_position2d(\n",
    "        select_exp(res, num_experiment),\n",
    "        boundary=plot_boundary,\n",
    "        label=f\"{name}\",\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax = plot_targets2d(\n",
    "        env,\n",
    "        boundary=plot_boundary,\n",
    "        color=\"black\",\n",
    "        alpha=1,\n",
    "        label=\"Targets\",\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "source": [
    "### Death"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [sniff_exp, chemo_exp, info_exp, rand_exp, rl_exp, wsls_exp]\n",
    "names = [\"Sniff\", \"Chemo\", \"Info\", \"Rando\", \"RL\", \"WSLS\"]\n",
    "colors = [\"purple\", \"blue\", \"green\", \"grey\", \"orange\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "source": [
    "### Total reward"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [sniff_exp, chemo_exp, info_exp, rand_exp, rl_exp, wsls_exp]\n",
    "names = [\"Sniff\", \"Chemo\", \"Info\", \"Rando\", \"RL\", \"WSLS\"]\n",
    "colors = [\"purple\", \"blue\", \"green\", \"grey\", \"orange\", \"orangered\"]\n",
    "\n",
    "# Score \n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "# fig = plt.figure(figsize=(7, 5))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "### Change the world!\n",
    "I am giving you three parameters (aka levers) which can change which agent dominates the others. In the above reference, for example, RL and WSLS dominate.\n",
    "\n",
    "By dominate I mean has:\n",
    "\n",
    "1. The most total reward\n",
    "2. Not the most deaths (a weaker criterion)\n",
    "\n",
    "The parameters, and the acceptable ranges, are:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_targets = (1, 1000)  # these are the allowed bounds\n",
    "noise_sigma = (0.0, 10)\n",
    "cog_mult = (1, 10)"
   ]
  },
  {
   "source": [
    "### Assignment\n",
    "Use the reference code below to answer the questions which follow it. That is, along with the plotting functions used throughout.\n",
    "\n",
    "If you cannot find an env that let's the agent in question dominate, report the best results you can."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Reference code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Change me\n",
    "num_targets = 20 \n",
    "cog_mult = 1\n",
    "noise_sigma = 2.0\n",
    "\n",
    "# ---\n",
    "# Shared (leave alone)\n",
    "num_experiments = 100\n",
    "num_steps = 200\n",
    "seed_value = 5838\n",
    "detection_radius = 1\n",
    "p_scent = 0.1\n",
    "max_steps = 1\n",
    "min_length = 1\n",
    "target_boundary = (10, 10)\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "scents = []\n",
    "for _ in range(len(targets)):\n",
    "    coord, scent = create_grid_scent_patches(\n",
    "        target_boundary, p=1.0, amplitude=1, sigma=2)\n",
    "    scents.append(scent)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents\n",
    "\n",
    "# rando\n",
    "diff = DiffusionGrid(min_length=min_length, scale=1)\n",
    "diff.seed(seed_value)\n",
    "\n",
    "# sniff\n",
    "sniff = GradientDiffusionGrid(\n",
    "    min_length=min_length, \n",
    "    scale=1.0, \n",
    "    p_neg=1, \n",
    "    p_pos=0.0\n",
    ")\n",
    "sniff.seed(seed_value)\n",
    "\n",
    "# smart chemo\n",
    "chemo = AccumulatorGradientGrid(\n",
    "    min_length=min_length, \n",
    "    max_steps=max_steps, \n",
    "    drift_rate=1, \n",
    "    threshold=3,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "chemo.seed(seed_value)\n",
    "\n",
    "# smart info\n",
    "info = AccumulatorInfoGrid(\n",
    "    min_length=min_length, \n",
    "    max_steps=max_steps, \n",
    "    drift_rate=1, \n",
    "    threshold=3,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "info.seed(seed_value)\n",
    "\n",
    "# RL\n",
    "critic = CriticGrid(default_value=0.5)\n",
    "actor = SoftmaxActor(num_actions=4, actions=possible_actions, beta=4)\n",
    "rl = ActorCriticGrid(actor, critic, lr=0.1, gamma=0.1)\n",
    "\n",
    "# WSLS\n",
    "possible_actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "num_action = len(possible_actions)\n",
    "initial_bins = np.linspace(0, 1, 10)\n",
    "\n",
    "critic_R = CriticGrid(default_value=0.5)\n",
    "critic_E = CriticGrid(default_value=np.log(num_action))\n",
    "actor_R = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20) \n",
    "actor_E = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20)\n",
    "\n",
    "wsls = WSLSGrid(\n",
    "    actor_E,\n",
    "    critic_E,\n",
    "    actor_R,\n",
    "    critic_R,\n",
    "    initial_bins,\n",
    "    lr=0.1,\n",
    "    gamma=0.1,\n",
    "    boredom=0.0\n",
    ")\n",
    "\n",
    "# !\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "sniff_exp = experiment(\n",
    "    f\"sniff\",\n",
    "    sniff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "chemo_exp = experiment(\n",
    "    f\"chemo\",\n",
    "    chemo,\n",
    "    env,\n",
    "    num_steps=num_steps * cog_mult,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "info_exp = experiment(\n",
    "    f\"info\",\n",
    "    info,\n",
    "    env,\n",
    "    num_steps=num_steps * cog_mult,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "rl_exp = experiment(\n",
    "    f\"rl\",\n",
    "    rl,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "wsls_exp = experiment(\n",
    "    f\"wsls\",\n",
    "    wsls,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "source": [
    "#### Question 2.1\n",
    "What enviromental parameters, from the ranges just given above, lead the rando agent to dominate the others. AKA _DiffusionGrid_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your best code/result here. \n",
    "# To prove domination - show me bar plots, and distribution plots to make your case!"
   ]
  },
  {
   "source": [
    "#### Question 2.2\n",
    "Explain **why** you think these parameters were best. Or, if you could not make the agent dominate, explain **why** you could not as best you can."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 2.3\n",
    "What enviromental parameters, from the ranges just given above, lead the sniff! agent to dominate the others. AKA _GradientDiffusionGrid_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your best code/result here. \n",
    "# To prove domination - show me bar plots, and distribution plots to make your case!"
   ]
  },
  {
   "source": [
    "#### Question 2.4\n",
    "Explain **why** you think these parameters were best. Or, if you could not make the agent dominate, explain **why** you could not as best you can."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 2.5\n",
    "What enviromental parameters, from the ranges just given above, lead the smart-chemo agent to dominate the others. AKA _AccumulatorGradientGrid_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your best code/result here. \n",
    "# To prove domination - show me bar plots, and distribution plots to make your case!"
   ]
  },
  {
   "source": [
    "#### Question 2.6\n",
    "Explain **why** you think these parameters were best. Or, if you could not make the agent dominate, explain **why** you could not as best you can."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 2.7\n",
    "What enviromental parameters, from the ranges just given above, lead the smart-info agent to dominate the others. AKA _AccumulatorInfoGrid_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your best code/result here. \n",
    "# To prove domination - show me bar plots, and distribution plots to make your case!"
   ]
  },
  {
   "source": [
    "#### Question 2.8\n",
    "Explain **why** you think these parameters were best. Or, if you could not make the agent dominate, explain **why** you could not as best you can."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  }
 ]
}