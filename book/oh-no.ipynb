{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('py3.6': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5c0fa7a4f8f1487a2aac67eb43e7b2e553808a81f9be50af9e1ab194481cfe22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ...Our dilemma, and three ways to reward - Lab\n",
    "\n",
    "In this assignment we study exploration, with a cliff. We will take advantage of the simulator in _The Paths Perspective on Value Learning_ to ask some basic but important questions about how learning rules, and exploration interact. \n",
    "\n",
    "The learning rate $\\alpha$ is fixed. All we can control is the degree of exploration ($\\epsilon$), with a nice little slider.\n",
    "\n",
    "Our agents of interest are Monte Carlo, SARSA, and Q-learning. These are our three ways to the most reward.\n",
    "\n",
    "The lab has two sections. \n",
    "\n",
    "- _First_ I quiz you with questions about both S & B, and the text in _The Paths Perspective on Value Learning_, and how they relate.\n",
    "\n",
    "- _Second_, is where we simulate exploration-exploitation and see how the same set of choices lead to quite different learning and interpretations of the same experiences (aka transition sets)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section 1 - Something about \"paths\"\n",
    "\n",
    "#### Question 1.1\n",
    "Like they did in S & B in Section 1, diagram out a tree diagram and planning route for the Cliff World, starting from the bottom left position.  Do this on a separate piece of paper, which you will take a picture of and upload later. Start off drawing out the game board, then imagine taking a random move, and write the tree of available choices. Keep going until you map out all the ways to get to the winning \"+2\" grid box.\n",
    "\n",
    "(_Note_: write small?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Upload your sketch on Canvas)"
   ]
  },
  {
   "source": [
    "#### Question 1.2\n",
    "Looking at the game \"from above\", when you can see all the paths at once, as it is shown in the article, makes the game look simple. After diagramming above, does the cliff game seem different to you? How challenging a game is it for our agents.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.3\n",
    "\n",
    "Put another way--how would you change the state representation to learn more of a \"birds eye view\", like the one you have when reading. Think about alternatives to state-as-a-single-location perhaps?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.4\n",
    "\n",
    "Discuss the difficulties your proposal in the last question might face? Computational explosions? Time limits? Bias? Tell me a downside or cost to your approach, and if you can how you might change things to overcome that?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.5\n",
    "Is the exploration problem easier or harder do you think for your scheme?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.6\n",
    "Provide an example of exploration and reward learning from your experience, or from the natural world at large, that fits well into the markov decision space abstract."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.1 \n",
    "HERE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  }
 ]
}