{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('py3.6': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5c0fa7a4f8f1487a2aac67eb43e7b2e553808a81f9be50af9e1ab194481cfe22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# The Oh My Information - Lab\n",
    "\n",
    "In this assignment we take on, or really return to _taxic exploration_. We revisit the sniff world (aka _ScentGrid_), with a new twist. What happens when sense information is not just noisy, but suddenly missing altogether. A concrete case of this is turbulent flows. \n",
    "\n",
    "We'll be working with a crude approximation of such flows. Our environment this tims just deletes scent information from the grid, with a probability $(1- p_{scent})$. The noisy background is of course unaffected by this deletion.\n",
    "\n",
    "The presence of this uncertainty makes decisions--of the kind common to decision theory--a necessity. So, we'll be using accumulator models, again. The decisions to be made are: \n",
    "\n",
    "- Q1: Is their a scent at all?\n",
    "- Q2: Is the gradient increasing or decreasing? \n",
    "\n",
    "Our target metrics are number of deaths, total reward, and best reward. Any experimental trial which does not lead to finding at least a single target (aka reward) means the exploring agent dies. It's a harsh noisy world we live in, after all. \n",
    "\n",
    "## E Coli, again?\n",
    "_Background_: Recall our basic model of E. Coli exploration is as simple as can be. \n",
    "\n",
    "- When the gradient is positive, meaning you are going \"up\" the gradient, the probability of turning is set to _p pos_. \n",
    "- When the gradient is negative, the turning probability is set to _p neg_. (See code below, for an example). \n",
    "- If the agent \"decides\" to turn, the direction it takes is uniform random.\n",
    "- The length of travel before the next turn decision is sampled from an exponential distribution just like the _DiffusionDiscrete_\n",
    "\n",
    "## Our agents, this time\n",
    "We will study three agents. One who does _chemotaxis_. One who does a kind of _infotaxis_. One that does random search (aka Diffusion). For fun, let's call this one a _randotaxis_ agent. (No one really calls them this.) This last rando-agent is really a control. A reference point.\n",
    "\n",
    "In a sense the _chemotaxis_ agent only tries to answer question Q2 (above). While _infotaxis_ only tries to answer Q1. They are extreme strategies, in other words. The bigger question we will ask, in a very limited setting, is which extreme methods is better _generally_? \n",
    "\n",
    "We will not address the question of combining them, into a less extreme and perhaps more reliable alternative...\n",
    "\n",
    "## Costly cognition\n",
    "Both _chemo-_ and _infotaxis_ agents will use a DDM-style accumulator to try and make better decisions about the direction of the gradient. These decisions are of course statistical in nature. (We won't be tuning the accumulator parameters in this lab. Assume the parameters I give you, for the DDM, are \"good enough\".)\n",
    "\n",
    "For the _randotaxis_ agent number of steps means the number of steps or actions the agent takes. \n",
    "\n",
    "As in the _Air Quotes Lab_ we will assume that the steps are in a sense conserved. For the other two (accumulator) agents a step can mean two things. For accumulator agents a step can be spent sampling/weighing noisy scent evidence in the same location, or it can be spent moving to a new location.\n",
    "\n",
    "## What's scent look like (a definition of _chemotaxis_)?\n",
    "Our _chemotaxis_ agent (_AccumulatorGradientGrid_) tries to directly estimate the gradient $\\nabla$ in scent by comparing the level of scent at the last grid position it occupied to the current scent level ($o$). By last grid position here we mean the last grid position when it moved last. \n",
    "\n",
    "$$\\nabla \\approx o_t - o_{t-1}$$\n",
    "\n",
    "Because an accumulator is present, our chemo- sequentially tries to estimate this gradient by sampling the new current location, until the threshold is met.\n",
    "\n",
    " Chemo-accumulators have what we can think of as two cognitive or behavioral steps:\n",
    "\n",
    "1. Use an accumulator to (stabely) estimate the chemo gradient\n",
    "2. Use the gradient to make turning decisions\n",
    "\n",
    "## More information (a definition of _infotaxis_)?\n",
    "Compared to chemo- definition the definition of infotaxis is a little more involved. It has what we can think of as five cognitive or behavioral steps:\n",
    "\n",
    "1. Use an accumulator to (stabely) estimate if there is a scent or not. AKA hits and misses.\n",
    "2. Build a probability model of hits/misses (at every point)\n",
    "3. Measure information gained when probability model changes \n",
    "4. Measure the gradient of information gains\n",
    "5. Use the gradient to make turning decisions\n",
    "\n",
    "_Note_: Even though the info-accumulator is more complex, it can take advantage of missing scent information to drive its behavior. It can also use positive scent hits, of course, too.\n",
    "\n",
    "## Our TED talk moment\n",
    "The big question for the week is this - is it ever better to follow the scent gradient instead of following its information?\n",
    "\n",
    "## Install and import needed modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install explorationlib?\n",
    "!pip install --upgrade git+https://github.com/parenthetical-e/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import misc\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "# Vis - 1\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exp\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "# Agents\n",
    "from explorationlib.agent import GradientDiffusionGrid\n",
    "from explorationlib.agent import AccumulatorGradientGrid\n",
    "\n",
    "# Env\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "from explorationlib.local_gym import create_grid_scent\n",
    "from explorationlib.local_gym import uniform_targets\n",
    "from explorationlib.local_gym import constant_values\n",
    "\n",
    "# Vis - 2\n",
    "from explorationlib.plot import plot_position2d\n",
    "from explorationlib.plot import plot_length_hist\n",
    "from explorationlib.plot import plot_length\n",
    "from explorationlib.plot import plot_targets2d\n",
    "from explorationlib.plot import plot_scent_grid\n",
    "\n",
    "# Score\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import num_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.greedy=True\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = \"16\"\n",
    "\n",
    "# Dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "## Section 1 -  Intelligent costs, for fun\n",
    "### Random search versus Cognition, when information is often missing and noisy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}