{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('py3.6': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5c0fa7a4f8f1487a2aac67eb43e7b2e553808a81f9be50af9e1ab194481cfe22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# The Follow the Directed - Lab\n",
    "\n",
    "In this assignment we see what follows when one follows directions, in exploration. We see what happens when exploration is not totally random, but is directed by an information bonus of some kind. The two kinds are--no surprise--the UCB, and a new idea for us, novelty. They will be compared using a softmax Actor. Later on we will re-introduce the bounded sequential (pure) explorer from _Be Best_. \n",
    "\n",
    "Recall one of the readings this week:\n",
    "\n",
    "> Ng, A., Harada, D. & Russell, S. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning 278--287 (1999). \n",
    "\n",
    "Ng's point, or really the impact of his work on us, even if he never made the point directly, is that common bonuses used in directed exploration schemes, for humans and agents, can introduce a statistical bias to our ability to accurately measure the value of actions.\n",
    "\n",
    "AKA Does inductive bias imply statistical bias? Here, anyway. \n",
    "\n",
    "(_Note_: This is not true in general, or for all systems. It _is_ sometimes true, and it is very important to know when.) \n",
    "\n",
    "The reward value $Q$ update rule for all agents (below):\n",
    "\n",
    "$$ Q \\leftarrow Q + \\alpha * (\\hat R - Q) $$\n",
    "\n",
    "where $\\hat R = R + \\lambda I$ and $I$ is a standin for the information bonus. The equation for UCB follows. In it $t$ is the number of steps in an episode, and $N_a$ is the number of times that action has been taken. \n",
    "\n",
    "$$\\sqrt{log(t)/ N_a)}$$ \n",
    "\n",
    "If an action hasn't been tried very often, or not at all, the $N_a$ will be small and the uncertainty which UCB reflects will be large, or relatively large anyway. The larger the UCB is, when it is treated as an intrinsic reward, the more likely that action will be accepted.\n",
    "\n",
    "There is not real reason to write an equation for the novelty bonus. It works like this. If this is the right time taking that action, add a $\\lambda$ to the reward $R$, in the update rule above. Otherwise, do nothing. THis one time bonus littera vale everywhere, making it likely all the actions will be explored, at least a little. \n",
    "\n",
    "(_Pssssst_ - there is good evidence that such novelty signals exist in both human, monkey, and mouse brains).\n",
    "\n",
    "The action policy, aka the _Actor_, will use the softmax sampling policy, whose free parameter $\\beta$ controls how soft the softmax is. Larger values make it harder. Aka, morre like a pure max.\n",
    "\n",
    "The questions to answer this week are: \n",
    "\n",
    "1. Does “good” (inductive) bias cause bad (statistical) bias in two directed schemes? If so, how much?\n",
    "2. Does this bad bias even matter? Or really, can it matter in simple examples that look common enough in the real world?\n",
    "\n",
    "The setting is a four-bandit. Our last time with these robbers.[**] The lab has two sections.\n",
    "\n",
    "_First_ we get to know our new agents, comparing rewards to value error. Doing some tuning.\n",
    "\n",
    "_Second_ we test how much this matters by testing how well our agents recover when the best choice \"runs out\" of rewards and changes to become the worst choice.\n",
    "\n",
    "In this lab we use two metrics. The familiar _total reward_, and a new error (RMSE) that measures the difference between the true expected value of each action, to the values that were learned by each agent. Entropy will show up too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install explorationlib?\n",
    "!pip install --upgrade git+https://github.com/parenthetical-e/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Env\n",
    "import explorationlib\n",
    "from explorationlib.local_gym import BanditUniform4\n",
    "from explorationlib.local_gym import BanditChange4\n",
    "\n",
    "# Actors and critics\n",
    "from explorationlib.agent import BanditActorCritic\n",
    "from explorationlib.agent import Critic\n",
    "from explorationlib.agent import CriticUCB\n",
    "from explorationlib.agent import CriticNovelty\n",
    "from explorationlib.agent import SoftmaxActor\n",
    "from explorationlib.agent import DeterministicActor\n",
    "from explorationlib.agent import BoundedSequentialActor\n",
    "\n",
    "# Exp\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "# Metrics of interest\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import bandit_rmse\n",
    "from explorationlib.score import action_entropy\n",
    "\n",
    "# Vis\n",
    "from explorationlib.plot import plot_bandit\n",
    "from explorationlib.plot import plot_bandit_actions\n",
    "from explorationlib.plot import plot_bandit_critic\n",
    "from explorationlib.plot import plot_bandit_hist"
   ]
  },
  {
   "source": [
    "## Section 1 - The Directed\n",
    "### Soft explorations\n",
    "The bigger $\\beta$ in softmax exploration, the more greedy or exploitative, the agent will be. To build some intuition, let's look at some examples. Note: there is no bonus here. \n",
    "\n",
    "Our bandit for this demo is:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env\n",
    "seed_value = 60\n",
    "env = BanditUniform4()\n",
    "env.seed(seed_value)\n",
    "\n",
    "# -\n",
    "ax = plot_bandit(env, alpha=0.6)"
   ]
  },
  {
   "source": [
    "Let's plot example behavoir for three experiments, at some different levels of $\\beta$ (Don't stray from these, at least in the questions; play all you want of course)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 3\n",
    "num_steps = 4 * 60  # 60 steps / arm; a lot\n",
    "\n",
    "betas = [2, 4, 6, 8]  \n",
    "results = []\n",
    "for beta in betas:\n",
    "    # (SoftmaxActor is our general reference)\n",
    "    ref = BanditActorCritic(\n",
    "        SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "        Critic(num_inputs=env.num_arms)\n",
    "    )\n",
    "    # !\n",
    "    log = experiment(\n",
    "        f\"demo_{beta}\",\n",
    "        ref,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "source": [
    "Visualize the effect of beta, one experiment per cell (three cells). Look over them all, please.\n",
    "\n",
    "#### Experiment 0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiment = 0\n",
    "for name, res in zip(betas, results):\n",
    "    plot_bandit_actions(\n",
    "        select_exp(res, num_experiment), \n",
    "        max_steps=num_steps,\n",
    "        s=4,\n",
    "        title=f\"Beta: {name} (N={num_experiment})\", \n",
    "        color=\"black\",\n",
    "        figsize=(6,2)\n",
    "    )"
   ]
  },
  {
   "source": [
    "#### Experiment 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiment = 1\n",
    "for name, res in zip(betas, results):\n",
    "    plot_bandit_actions(\n",
    "        select_exp(res, num_experiment), \n",
    "        max_steps=num_steps,\n",
    "        s=4,\n",
    "        title=f\"Beta: {name} (N={num_experiment})\", \n",
    "        color=\"black\",\n",
    "        figsize=(6,2)\n",
    "    )"
   ]
  },
  {
   "source": [
    "#### Experiment 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiment = 2\n",
    "for name, res in zip(betas, results):\n",
    "    plot_bandit_actions(\n",
    "        select_exp(res, num_experiment), \n",
    "        max_steps=num_steps,\n",
    "        s=4,\n",
    "        title=f\"Beta: {name} (N={num_experiment})\", \n",
    "        color=\"black\",\n",
    "        figsize=(6,2)\n",
    "    )"
   ]
  },
  {
   "source": [
    "#### Question 1.1\n",
    "In the paper below, the authors offered evidence that people tune their use of information bonuses, and their level of noise, depending on the task, the information available, and the horizon for exploration they have to work with. So...\n",
    "\n",
    "To prove the point, let's do the opposite. Running no more simulations, and knowing nothing about how much reward the above returned, make a best guess for the $\\beta$ value you believe can be a fair and robust choice for the entire lab to come. Keep in mind the brief description I gave you of Section 2, and the exploration bonuses will be in play soon.\n",
    "\n",
    "_Hint_: Arm 2 is the most valuable choice. It is the \"best\" arm.\n",
    "\n",
    "_Warning_: Don't cheat. Make a guess. Explain why. And you'll do fine grading wise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer below as a code cell. Explain your choice here as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0 # change me? (My choice of 1.0 is not a hint. I picked it at random."
   ]
  },
  {
   "source": [
    "Let's compare performance between the reference _SoftmaxActor_ using a plain old _Critic_, and a critic using UCB bonuses _CriticUCB_ and a _CriticNovelty_ bonus agent?\n",
    "\n",
    "#### Question 1.2\n",
    "Make a guess, will adding a bonus increase of decrease the total rewards collected compared to the raw critic?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.3\n",
    "Make a guess, will _CriticUCB_ or _CriticNovelty_ do better?\n",
    "\n",
    "Hint: _CriticUCB_ provides a running bonus of how uncertain we may be about each arm. _CriticNovelty_ is a one-off bonus at the start. Before answering, consider how much of a hint an agent may need on this task, and how much noise you are using (how small your beta is)? \n",
    "\n",
    "Consider the factors in the hint in explaining your answer?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Well, let's see what happens...."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "bonus_weight = 0.5\n",
    "\n",
    "# Agents\n",
    "ref = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "# UCB\n",
    "ucb = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticUCB(num_inputs=env.num_arms, bonus_weight=bonus_weight)\n",
    ")\n",
    "# Novely\n",
    "nov = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticNovelty(\n",
    "        num_inputs=env.num_arms, \n",
    "        novelty_bonus=1.0,\n",
    "        bonus_weight=bonus_weight\n",
    "    )\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [ref, ucb, nov]\n",
    "names = [\"softmax\", \"softmax-ucb\", \"softmax-nov\"]\n",
    "colors = [\"blue\", \"green\", \"purple\"]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "source": [
    "#### Example behave"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiment = 10\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    plot_bandit_actions(\n",
    "        select_exp(res, num_experiment), \n",
    "        max_steps=120,\n",
    "        s=4,\n",
    "        title=name, \n",
    "        color=color,\n",
    "        figsize=(6,2)\n",
    "    )"
   ]
  },
  {
   "source": [
    "#### Value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=list(range(0, num_steps, 2)))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Total reward\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### Question 1.4\n",
    "Which of the agents in Q1.3 will have the most entropy, and how will this relate the distribution of _total reward_ we plotted in that question?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's see!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = action_entropy(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=np.linspace(0, 1.5, 50))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Entopy\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### Question 1.5\n",
    "Given we are holding noise ($\\beta$) constant, is it fair or unfair to consider entropy here to be a direct measure of the \"directedness\" of the exploration? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's increase the purity of our experiments. Let's add in the _BoundedSequentialActor_ with a normal _Critic_. I'll pick an ambitious _bound_ for us. No need to tune it. Assume I did a good and fair job in my choice\n",
    "\n",
    "#### Question 1.6\n",
    "Will _BoundedSequentialActor_ do better or worse than all the other intelligent direct agents we have been playing with? Make a guess, based on the results from the _Be Best_ lab."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's find out...."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_weight = 0.5\n",
    "\n",
    "# Agents\n",
    "ref = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "# UCB\n",
    "ucb = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticUCB(num_inputs=env.num_arms, bonus_weight=bonus_weight)\n",
    ")\n",
    "# Novely\n",
    "nov = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticNovelty(\n",
    "        num_inputs=env.num_arms, \n",
    "        novelty_bonus=1.0,\n",
    "        bonus_weight=bonus_weight\n",
    "    )\n",
    ")\n",
    "seq = BanditActorCritic(\n",
    "    BoundedSequentialActor(num_actions=env.num_arms, bound=20),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [ref, ucb, nov, seq]\n",
    "names = [\"softmax\", \"softmax-ucb\", \"softmax-nov\", \"b-sequential\"]\n",
    "colors = [\"blue\", \"green\", \"purple\", \"grey\"]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=list(range(0, num_steps, 2)))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Total reward\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### Question 1.7\n",
    "Given the results in Q1.6, when we plot entropy below do you think you should revise your prediction for how entropy and total rewards relate? If you do revise it, explain."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's find out...."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = action_entropy(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=np.linspace(0, 1.5, 50))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Entopy\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### Question 1.8\n",
    "Were you right? If not, please try and explain why.\n",
    "\n",
    "Also, was the entropy of the _BoundedSequentialActor_ more or less than you expected? Do you understand why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your two answers here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's measure the error for the values we learned in Q1.7. \n",
    "\n",
    "#### Question 1.9\n",
    "Rank the models you expect to have the most error, to the least error. If you think one, or more, models will be about the same, that is ok.\n",
    "\n",
    "One answer to this question could be _BoundedSequentialActor_ > _Critic_ = _CriticUCB_ = _CriticNovelty_, but this is not the right answer. Just helping you with the form I want the answer to take."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "#### Question 1.10\n",
    "Beyond a simple ranking, how do you think the differences in error will scale evenly with the difference in total reward, which often are not that large in these kinds of simple bandit experiments. \n",
    "\n",
    "Will the change in error be about linear with total reward, or a lot more, or a lot less?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's find out, by measure the RMSE between the bandits true value and what each agent believes about it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = bandit_rmse(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Error\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "bins = np.linspace(0, np.max(scores), 40)\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "for (name, s, c) in zip(names, scores, colors):    \n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=bins)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Error\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### Question 1.11\n",
    "Does your answer to Q1.10 look about right? If your guess was off, please try and explain why."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's make one big change! Let's change the horizon from 240 steps, to 60, and see how that changes total reward, and error.\n",
    "\n",
    "#### Question 1.12\n",
    "Do you think a change to horizon will affect the ranking of the models? Why or why not?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 60\n",
    "bonus_weight = 0.5\n",
    "\n",
    "# Agents\n",
    "ref = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "# UCB\n",
    "ucb = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticUCB(num_inputs=env.num_arms, bonus_weight=bonus_weight)\n",
    ")\n",
    "# Novely\n",
    "nov = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticNovelty(\n",
    "        num_inputs=env.num_arms, \n",
    "        novelty_bonus=1.0,\n",
    "        bonus_weight=bonus_weight\n",
    "    )\n",
    ")\n",
    "seq = BanditActorCritic(\n",
    "    BoundedSequentialActor(num_actions=env.num_arms, bound=20),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [ref, ucb, nov, seq]\n",
    "names = [\"softmax\", \"softmax-ucb\", \"softmax-nov\", \"b-sequential\"]\n",
    "colors = [\"blue\", \"green\", \"purple\", \"grey\"]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=list(range(0, num_steps, 2)))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Total reward\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = bandit_rmse(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Error\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "bins = np.linspace(0, np.max(scores), 40)\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "for (name, s, c) in zip(names, scores, colors):    \n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=bins)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Error\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### Question 1.14\n",
    "Was your answer to Q1.13 correct? Why or why not?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "## Section 2 - the world changes\n",
    "### Can we recover when the best rewards run out?\n",
    "\n",
    "In this section our task will start off the same as in Section 1. After 60 steps however, the best arm (arm 2) will become the worst. The question is who can recover? And does that recovery matter if they did a really good job collecting reward before the changes\n",
    "\n",
    "#### Question 2.1\n",
    "Keeping in mind the change to come, which agent will do the best overall? \n",
    "\n",
    "To answer first imagine we run 80 steps in total, 20 steps past the change. Then imagine we run 120 steps in total. In the third part of your answer imagine we run 240 steps (this is the amount we have been using so far. It is 180 steps past the change point, when the best reward runs out)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  },
  {
   "source": [
    "Let's find out. Not the values below of the Env before and after then change...."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment settings\n",
    "# For all keep the maze the saame\n",
    "num_experiments = 100\n",
    "num_change = 60\n",
    "seed_value = 60\n",
    "\n",
    "# Env\n",
    "env = BanditChange4(num_change=num_change)\n",
    "env.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bandit(env.orginal, alpha=0.6, title=\"Original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bandit(env.change, alpha=0.6, title=\"Change\")"
   ]
  },
  {
   "source": [
    "#### 80 steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 80\n",
    "bonus_weight = 0.5\n",
    "\n",
    "# Agents\n",
    "ref = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "# UCB\n",
    "ucb = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticUCB(num_inputs=env.num_arms, bonus_weight=bonus_weight)\n",
    ")\n",
    "# Novely\n",
    "nov = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticNovelty(\n",
    "        num_inputs=env.num_arms, \n",
    "        novelty_bonus=1.0,\n",
    "        bonus_weight=bonus_weight\n",
    "    )\n",
    ")\n",
    "seq = BanditActorCritic(\n",
    "    BoundedSequentialActor(num_actions=env.num_arms, bound=20),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [ref, ucb, nov, seq]\n",
    "names = [\"softmax\", \"softmax-ucb\", \"softmax-nov\", \"b-sequential\"]\n",
    "colors = [\"blue\", \"green\", \"purple\", \"grey\"]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=list(range(0, num_steps, 2)))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Total reward\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### 120 steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 120\n",
    "bonus_weight = 0.5\n",
    "\n",
    "# Agents\n",
    "ref = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "# UCB\n",
    "ucb = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticUCB(num_inputs=env.num_arms, bonus_weight=bonus_weight)\n",
    ")\n",
    "# Novely\n",
    "nov = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticNovelty(\n",
    "        num_inputs=env.num_arms, \n",
    "        novelty_bonus=1.0,\n",
    "        bonus_weight=bonus_weight\n",
    "    )\n",
    ")\n",
    "seq = BanditActorCritic(\n",
    "    BoundedSequentialActor(num_actions=env.num_arms, bound=20),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [ref, ucb, nov, seq]\n",
    "names = [\"softmax\", \"softmax-ucb\", \"softmax-nov\", \"b-sequential\"]\n",
    "colors = [\"blue\", \"green\", \"purple\", \"grey\"]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=list(range(0, num_steps, 2)))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Total reward\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "#### 240 steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 240\n",
    "bonus_weight = 0.5\n",
    "\n",
    "# Agents\n",
    "ref = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "# UCB\n",
    "ucb = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticUCB(num_inputs=env.num_arms, bonus_weight=bonus_weight)\n",
    ")\n",
    "# Novely\n",
    "nov = BanditActorCritic(\n",
    "    SoftmaxActor(num_actions=env.num_arms, beta=beta),\n",
    "    CriticNovelty(\n",
    "        num_inputs=env.num_arms, \n",
    "        novelty_bonus=1.0,\n",
    "        bonus_weight=bonus_weight\n",
    "    )\n",
    ")\n",
    "seq = BanditActorCritic(\n",
    "    BoundedSequentialActor(num_actions=env.num_arms, bound=20),\n",
    "    Critic(num_inputs=env.num_arms)\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [ref, ucb, nov, seq]\n",
    "names = [\"softmax\", \"softmax-ucb\", \"softmax-nov\", \"b-sequential\"]\n",
    "colors = [\"blue\", \"green\", \"purple\", \"grey\"]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)   \n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=list(range(0, num_steps, 2)))\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Total reward\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "source": [
    "This lab has shown you directed exploration can improve total rewards collected, but at the cost of sometimes large errors, and thaat sometimes this error can limit performance on long horizons, when the world changes.\n",
    "\n",
    "#### Question 2.2\n",
    "Imagine in our final question that you, intelligent agent you are, could pick and choose among these four agents to be your strategies, but in an adaptive way and as you see fit. Please write down for a situation in which each of the agents might be the best choice. \n",
    "\n",
    "These examples could be in bandit tasks, in a cliff worlds, or in open fields. Integrate the environments, in other words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here as comment. Explain yourself."
   ]
  }
 ]
}